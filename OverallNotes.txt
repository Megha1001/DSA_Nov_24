Asymptotic Ananlysis
----------------------
-> The idea is to measure order of growth. (We meaure order of growth of time taken by a functional program, in terms of input size.)
-> Doesn't depend upon machine, programming language etc.
-> No need to implement, we can analyze algorithm.

Assumptions
------------
-> Mathematical operations like addition, substraction, division, multiplication etc taken same time (whether you add 1+1 or 10000000+100000000, its consider to take same time)


Direct way to find and compare growths
------------------------------------------
- Ignore lower order terms
- Ignore leading term constant

How to compare terms ?
-----------------------
c < logLogN < logN < N^1/3 < N^1/2 < N < N^2 < N^3 < N^4 < 2^N < N^N

------------
The algorithm whose Order of growth is higher is bad algorithm
------------

Asymptotic Notation
--------------------
Big O : Exact or Upper
Theta : Exact
Omega : Exact or lower

------
When we know exact order of growth use Theta notation not Big O

------

Recursion Tree Method for solving reccurrences
- We consider the recursion tree and compute total work done.
- We write non recursive part as root of the tree and write the recursive part as children
- We keep expanding until we see a pattern

-------

Space complexity 
- Order of growth of memory ( or RAM) space in terms of input size

Auxiliary Space
- Order of growth of extra space or temporary space in terms of input size. (space created other than input and output)
    - Stack call consideration
- Its basically equals to the height of the tree ( maximum length from root to leaf path )



------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
Euclidean Algorithm
---------------------
1. Let 'b' be smaller than 'a'
     gcd(a, b) = gcd(a-b, b)

why ? 
    Let 'g' e GCD of 'a' and 'b'
    a = gx , b = gy and GCD(x, y) = 1
    (a-b) = g(x-y) ==> GCD of 1-b has be to 'g' otherwise it must have some common factor b/w x, y that contradicts GCD(x, y)=1
    Hence 
    gcd(a, b) = gcd(a-b, b) => g

Optimized euclidean algorith --> Time Complexity : O(log(min(a,b)))
Given number -> a,b
    if(b==0) return a;

    return gcd(b, a%b)


--------
LCM Algo
LCM is the smallest number that divides both the numbers.
a*b = LCM(a, b) * gcd(a, b)
---------

Prime number : Number that are only divisible by 1 and itself
Note : 1 is not the prime number ( nor composite number(non prime are termed as composite))
2 is the only even prime number


Idea : Divisors always appears in pair

30 -> (1,30) , (2, 15), (3,10), (5, 6)

=> if (x,y) is a pair
    x*y = n 
    and if x<=y
    x*x <=n
    => x <= Sqrt(n)

- Sieve of Eratosthenes

-----------------
Computing power

power(x,n) = power(x,n/2)*power(x,n/2), if n is even
power(x, n) = power(x,n/2)*power(x,n/2) * x, if n is odd

---------------
BitWise Operators
--------------------
1. Negation/ Bitwise not(~)
Positive numbers -> represent in decimal -> binary
Negative numbers -> represet in 2's complement


2's complement representation of -x = 2^32 - x


all 1s = 2^32-1


left shift operator(<<)
------------------
Signed and unsigned -> doesn't matter
Operation: The left shift operator simply shifts bits to the left by a specified number of positions.
Effect: Each left shift effectively multiplies the number by 2, regardless of whether the number is signed or unsigned.
No Sign Consideration: Since left shifting always adds zeros to the right, it does not affect the sign bit. As a result, the outcome is the same for both signed and unsigned integers when using the left shift operator.


Right Shift Operator (>>)
--------------------------
Syntax: value >> n
Function: Shifts the bits of value to the right by n positions. Each shift to the right effectively divides the number by 2.

Signed vs. Unsigned:

Signed Right Shift: Preserves the sign of the number. For a negative number, it fills the leftmost bits with 1.
Example: For a signed integer, -4 >> 1 (binary 11111100 for -4 in 8-bit) becomes -2 (binary 11111110).

Unsigned Right Shift: Does not preserve the sign. It always fills the leftmost bits with 0, regardless of the original sign.
Example: For an unsigned integer, -4 >> 1 (interpreting -4 as 252 in 8-bit) becomes 126 (binary 01111110).

Unsigned Right Shift Operator (>>>)
Syntax: value >>> n
Function: Performs a right shift but always fills the leftmost bits with 0, treating the number as an unsigned integer.
Example: In Java, if -1 >>> 1 results in 2147483647 (binary 01111111111111111111111111111111).

Summary
Left Shift (<<): Multiplies by powers of two.
Right Shift (>>): Preserves sign for signed integers; divides by powers of two.
Unsigned Right Shift (>>>): Ignores sign, always fills with 0.


----------------

Binary representation of -ve numbers
- Negative numbers are represented in 2's complement form

- > Range of numbers : [-2^(n-1) to 2^(n-1) - 1], here n are number of bits

-> steps to find 2's complement : 
    a. invert all the bits
    b. add 1
    Direct formula = 2^n - x

-> Easiest way Steps to Find 2's Complement by Traversing:)
Start from the leftmost bit and traverse the binary number to find the first occurrence of 1.
Copy all bits as they are until you encounter the first 1.


Why 2's complement form
-ve numbers are represented using 2's complement form . +ve numbers uses simple decimal to binary conversion

with other available representation(signed bit, 1's complement -> we have two representation for zero, in 2's complement we have only one representation of zero)


-> Signed Number -> Range 0 to 2^n -1 -> Support by C++, Python, C,  (supports both signed and unsigned)
-> Unsigned Number -> -2^(n-1) to 2^(n-1) -1 --> Support by JAVA

REASON 1 :-> We have only one representation of zero
REASON 2 :-> Intuition behind 2's complement
    -> THe arithmetic operators are easier to perform. Actually 2's complment form is derived from the idea of 0-x

REASON 3 :-> The leading bit is always zero( here we are only talking about -ve numbers, for +ve we are anyway using n-1 bit so leading bit is always 0)


---------------------------------------------
                    RECURSION
---------------------------------------------
If we have solution to smaller problems we can use those solution to solve the bigger problem.



TAIL RECURSION : A function is called tail recursive when the parent function has nothin gmore to do when the child function finish its execution.
In simple words, a  function is tail recursive when the last thing that happens in the function is recursive call and nothing happens after that.

Ex :
void fun(int n){
    if(n==0){
        return;
    }
    print(n)
    fun(n-1);
}

Why is this function execution faster ?
The point is, your caller doesn't have to save the state because in recursion. So what modern compilers do if they see function is tail recursive they change the function 


void fun(int n){
    start:
    if(n==0){
        return;
    }
    print(n)
    n = n-1;
    goto start
}

These are the changes they make internally --> they remove recursive completely. The changes that modern compiler makes is call TAIL CALL ELIMINATION

Notes
 : Quick Sort is tail recursive and merge sort is not . That is one of the reason why Quick sort is preferred
 : We can't convert every recursive to tail recursive
 : Tree Traversal
    : PreOrder traversal : tail recursive for right child
    : InOrder traversal : tail recursive for right child
    : PostOrder traversal : not tail recursive
    If we have a choice to do question then we should always try to do that using either preorder or inorder traversal since these are tail recursive

# Base Case
Base cases in recursion are those cases/inputs for which we can't break the problem into smaller such problems

------------------
Subsets/subsequences : String obtained by removing 0 or more characters from the given string and keeping the order of remaning characs as it is.

For the length of string n there can be 2^n subsets

------------------------------------------------------------
########################ARRAY########################
------------------------------------------------------------

- Contiguous memory
- accessed using index, that begin with 0.

Advantages
- Random access (Since elements are stored at contiguous location, we can get ith element in constant time, arr[i])
- Cache Friendliness

Types
- Fixed sized array

- Dynamic sized array : ArrayList

Note : The elements of an ArrayList are not stored in contiguous locations, 
Memory allocation for both fixed and dynamic size array is heap allocated

Its always recommened to use dynamic size array (You can initialize the initial size)

------------------------------------------------------------------------------------------------------------------------
ARRAY
------------------------------------------------------------------------------------------------------------------------

INSERT : O(N)
SEARCH : O(N) -> FOR UNSORTED
         O(logN) -> For sorted using binary search
DELETE : O(N)

Get ith element : O(1) -> random access
Update ith elelment : O(1) -> random access

Note : Insert at end and delete from end can be done in O(1) time


------------------------------------------------------------------------------------------------------------------------
ARRAYLIST
------------------------------------------------------------------------------------------------------------------------
Advantages over normal/fixed sized arrays
- Dynamic Size
- Rich library function

- Insert at end in Dynamic sized array : (Amortized time complexity) : O(N)
Amortized time complexity refere to the average time complexity per operation over a sequence of operations

------------------------------------------------------------------------------------------------------------------------
SORTING
------------------------------------------------------------------------------------------------------------------------

General Purpose
- Merge Sort
- Heap Sort
- Quick Sort

Hybrid Sorting algorithm
- Tim sort in python = insertion sort + merge sort
Generally : use merge sort, if the input becomes small during recursive calls or input is small then it switch to insertion sort

- IntroSort : Quick Sort, Heap Sort, Insertion
https://www.geeksforgeeks.org/introsort-or-introspective-sort/


QuickSort is not stable sort , in C++ we have stable_sort function that uses Merge sort instead of quick
A stable sorting algorithm maintains the relative order of items with equal keys in a sorted list

-----
### **Notes on Sorting Algorithms**

Sorting algorithms are fundamental in computer science and are used to arrange data in a specific order (ascending or descending). Here are key concepts, classifications, and examples to help understand them better.

---

### **1. Characteristics of Sorting Algorithms**
#### **a. Time Complexity**
- **Best Case**: Minimum time the algorithm takes (e.g., already sorted data).
- **Worst Case**: Maximum time the algorithm takes (e.g., reverse order).
- **Average Case**: Expected time for random input data.
  
| Algorithm      | Best Case   | Average Case | Worst Case   |
|----------------|-------------|--------------|--------------|
| Bubble Sort    | O(n)        | O(n²)        | O(n²)        |
| Quick Sort     | O(n log n)  | O(n log n)   | O(n²)        |
| Merge Sort     | O(n log n)  | O(n log n)   | O(n log n)   |
| Insertion Sort | O(n)        | O(n²)        | O(n²)        |
| Selection Sort | O(n²)       | O(n²)        | O(n²)        |

---

#### **b. Space Complexity**
- Measures the auxiliary space or temporary storage required.
- **In-place Algorithms**: Use O(1) extra space (e.g., QuickSort, Bubble Sort).
- **Not In-place Algorithms**: Use more than O(1) extra space (e.g., MergeSort).

---

#### **c. Stability**
- A **stable algorithm** preserves the relative order of records with equal keys.
  - Example: If two elements `A` and `B` with the same value appear in the input, `A` appears before `B` in the sorted output.
- Examples of stable algorithms: MergeSort, BubbleSort.
- Examples of unstable algorithms: QuickSort, HeapSort.

---

### **2. Common Sorting Algorithms**

#### **a. Simple/Elementary Sorting Algorithms**
- **Bubble Sort**: Repeatedly swaps adjacent elements if they are in the wrong order.
- **Selection Sort**: Selects the smallest element in each iteration and places it in the correct position.
- **Insertion Sort**: Builds the sorted array one item at a time by inserting elements into their correct position.

#### **b. Divide and Conquer Algorithms**
- **Quick Sort**: Divides the array using a pivot and sorts the partitions recursively.
- **Merge Sort**: Divides the array into halves, sorts them, and merges the sorted halves.

#### **c. Non-comparison-Based Algorithms**
- **Radix Sort**: Sorts numbers digit by digit starting from the least significant to the most significant.
- **Counting Sort**: Counts the occurrences of each element and uses this count to place elements in order.
- **Bucket Sort**: Distributes elements into buckets and sorts each bucket individually.

---

### **3. Applications of Sorting**
- **Searching**: Sorted data enables faster search (e.g., binary search).
- **Data Analysis**: Sorting is crucial for organizing and analyzing datasets.
- **Graph Algorithms**: Used in Kruskal’s and Prim’s algorithms for finding Minimum Spanning Trees.
- **String Processing**: Sorting strings or suffix arrays is used in text matching algorithms.

---

### **4. Tips for Choosing a Sorting Algorithm**
1. **Small Input Size**: Use simple algorithms like Insertion Sort or Bubble Sort.
2. **Large Input Size**: Use efficient algorithms like Merge Sort or Quick Sort.
3. **Stable Sorting Required**: Use Merge Sort or Bubble Sort.
4. **Space Constraints**: Prefer in-place algorithms like QuickSort or HeapSort.
5. **Known Value Range**: Use Counting Sort or Radix Sort.

---

### **5. Sorting Techniques Overview**
| **Algorithm**     | **In-place?** | **Stable?** | **Use Case**                                    |
|--------------------|---------------|-------------|------------------------------------------------|
| Bubble Sort        | Yes           | Yes         | Small datasets, educational purposes.          |
| Selection Sort     | Yes           | No          | Small datasets, when stability isn't required. |
| Insertion Sort     | Yes           | Yes         | Nearly sorted datasets.                        |
| Quick Sort         | Yes           | No          | General-purpose, when space is limited.        |
| Merge Sort         | No            | Yes         | Large datasets, requires stability.            |
| Heap Sort          | Yes           | No          | Large datasets, memory constraints.            |
| Counting Sort      | No            | Yes         | Small range of integers.                       |
| Radix Sort         | No            | Yes         | Sorting large integers or strings.             |

---

### **6. Key Takeaways**
- Choose the algorithm based on the input size, memory constraints, and stability requirements.
- Divide-and-conquer algorithms are powerful for larger datasets.
- Non-comparison-based algorithms are efficient for specific scenarios like sorting integers.


------------------
Sorting in java
    - Arrays
        -> Primitive --> Stablility not needed -> Uses Dual Pivot Quick Sort
        -> Non Primitive -> Integer, String, Object, Student --> Stability needed -> Based on merge sort adaptation of TimSort
    - Collection    -> that implements list interface on those we can apply Collection.sort function --> Needs stability -> Based on Merge Sort adaptation of TimSort

- Java uses stable algorithm when non primitive is sorted and uses non stable algorithm when primitive needs to be sorted(unstable is slightly faster)


----------
Arrays.sort
    -> for primitive types done sorting in natural order only (increasing or non decreasing) it doesn't allow comparator --> BUT THIS IS FOR PRIMITIVE TYPE ONLY
    -> FOR NON PRIMITIVE it does allow to provide comparator

----
Stable sorting algorithm : (IBM) : Insertion, Bubble and Merge Sort
Unstable sorting algorithm : Selection Sort, Quick Sort , Heap Sort


BUBBLE SORT
---------------
- One of the basic comparsion sorting technique.
- Compare consecutive element and swap only when arr[j] > arr[j+1] //not equal to keep it stable.
- In place, stable
- TIME COMPLEXITY : O(N^2), but if we use swapped flag variable otherwise Theta(N^2)

-----------
SELECTION SORT - Theta(N^2)
- One of the basic comparsion sorting technique
- Does less memory writes as compared with quicksort, merge sort , insertion sort etc. But CYCLE SORT is optimal in terms of memory writes
    MEMORY WRITES CAN BE A COSTLY OPERATION in terms of EEPROM
        -> For EEPROM, the age of the memory reduces as we write on it.
        -> Not only EEPROM, suppose we are swapping data on harddisk
- Its the basic idea of Heap Sort
    -> Heap sort is based on selection sort just that it uses heap to optimize the selection sort
-> Not stable
-> In place

BASIC IDEA : 
1. We find out the minimum element put at the first position
2. we find out the second minimum element put at the second position.

TIME COMPLEXITY : Theta(N^2)

------------
INSERTION SORT
- O(N^2) worst case
- In place and stable
- used in practice for small arrays(TimSort, IntroSort)
- O(N) best case -> In case of sorted array

Note : This algorithm has always preferred for small sized arrays

LOGIC : 
- We maintain the part already sorted
- We insert the current element in already sorted part
- We make the sorted part increment by 1 after every iteration

-------
MERGE SORT
-Divide and conquer algorithm (divide, conquer and merge)
- Stable algorithm
- Theta(NlogN) time and O(N) AUX Space
    Best time that we can get on single processor with random input
    There is a tweak algo of this -> Block sort that is inplace
- Well suited for linked list and works in O(1) aux space
- Used for external sorting
- In general for arrays, quicksort outperforms

 * TIME COMPLEXITY : Theta(N LOGN) = N work at every level * Number of levels
 * Analysis : At every level we are doing Theta(N) for merging two sorted arrays and there are log N levels

 * AUXILIARY SPACE : Theta(N), merge function need 
 *  -> This is not Theta(NlogN) because at any moment we need Theta(N) auxiliary space (other got deallocated)
 

----------------------------
QUICK SORT
----------------------------
Partition Alogs
    - Naive Partition : 3 traversal, Time complexity : O(N), Auxiliary space : O(N), Inplace
        -> Any element can be pivot
        -->  stable algo
    - Lomuto Partition : 1 traversal, Time Complexity : O(N), Auxiliary space : O(1), not in place
        -> last element must be pivot. If not swap the last element with pivot then apply algo
        --> Not stable algo
    - Hoare's partition : Normally performance better than lomuto partition
        -> Normally the pivot used in algo is first(low) element but partition has not been done around it. it will return the index of where first partition ends
         * It only ensure that elements from l to j is =< pivot and >=pivot on right side
         * ALso there is no gurantee that pivot is at correct position
            --> Not stable algo
    Normally , hoare's perform better in terms of number of comparision

Divide and conquer algorithm
worst case complexity : O(N^2)
Despite O(N^2) worst case, its considered faster because of the following reasons
    - In place
    - Cache friendly
    - Average case is O(NLogN)
    - Tail recursive
- Partition key function


-------------------
QuickSelect algorithm --> to find Kth largest and kth Smallest --> sorting/kthSmallestElement/FindKthSmallestElementUsingLomutoPartitionFunction.java

 * Logic : 
 * 1. Partition the array around the pivot.
 * 
 * 2. if p==k-1 return arr[p]
 * else if p > k-1 then high = p-1;
 * else low = p+1
 * 
 * This is QuickSelect Algorithm


QuickSelect is an algorithm used to find the  k-th smallest element in an unsorted array without fully sorting it.
    It works by partitioning the array around a pivot, ensuring:
        Elements smaller than the pivot are on the left.
        Elements larger than the pivot are on the right.
    The partition containing the k-th smallest element is partially sorted:
        Elements on the left of the k-th smallest are smaller.
        Elements on the right are larger.
    Only the necessary part of the array is processed, making it faster than full sorting for this purpose.


-----
Dutch National Flag Algorithm – One Pass – O(n) Time and O(1) Space
The problem is similar to “Segregate 0s and 1s in an array”. The idea is to sort the array of size n using three pointers: lo = 0, mid = 0 and hi = n – 1 such that the array is divided into three parts –


arr[0] to arr[lo – 1]: This part will have all the zeros.
arr[lo] to arr[mid – 1]: This part will have all the ones.
arr[hi + 1] to arr[n – 1]: This part will have all the twos.
Here, lo indicates the position where next 0 should be placed, mid is used to traverse through the array and hi indicates the position where next 2 should be placed.


Traverse over the array till mid <= hi, according to the value of arr[mid] we can have three cases:

arr[mid] = 0, then swap arr[lo] and arr[mid] and increment lo by 1 because all the zeros are till index lo – 1 and move to the next element so increment mid by 1.
arr[mid] = 1, then move to the next element so increment mid by 1.
arr[mid] = 2, then swap arr[mid] and arr[hi] and decrement hi by 1 because all the twos are from index hi + 1 to n – 1. Now, we don’t move to the next element because the element which is now at index mid can be a 0 and therefore needs to be checked again.


------
MATRIX

- In java , memory allocation for multi-dimensional array is different. They not need to be stored at contigous location (unlike C++ when 2D array/multi-dimensional arrays is contigous)
- We do not specify dimensions in java
    int arr[2][3] --> this is invalid
    int arr[][3] --> this is invalid

    Valid : 
        1. int arr[][] = {{1,3,4},{4,5, 6, 7}}; //assign and java will figure out
        2. int arr[][] = new int[3][4]; //first dimension is row and second is column


------
Jagged Array
Jagged arrays in java are arrays of arrays with varying sizes, allowing for dynamic memory allocation and efficient space utilization.

-------

HASHING
-------

- Insert, search , delete --> O(1) on average

Array takes atleast one operation with O(N)
BST --> AVL, red black tree -> O(logN)

Hashing can't found like key which is just smaller/bigger than provided key --> in tree we can do. with hashing its strict search. otherwise it will take O(N)/ costly operation.
we can't print sorted data

Hashing we can consider an expect data structure insert, delete, search not for sorted/closet find, prefix searching

Hashing is not useful for
    - Fiding closest value -> use AVL/RedBlack
    - sorted data -> use AVL/RedBlack
    - Prefix searching. --> Hashing does exact key search --> Use Trie

APPLICATIONS of hashing
------------------------
- It is widely used data structure(after array)

Dictionaries
Database indexing
Cryptography
Caches
Symbol tables in compilers/interpreters
Routers
Getting data from databases

Many more

----------
Direct Address Table
- It can't handle large value(Like phone numbers)
- Floating point number (We can't use these numbers as index of array)
- key are strings -> we can't use them as index in array for DAT
- any other char that is combination of numbers and characters --> we can't define index for those keys in array

TO overcome all these limitations , the Data structure comes to picture is hashing

Hashing Introduction
----------------------

[Universe of keys] --> Hash Function --> convert to value that can be used as index in array(called as hash Tables)

Universe of keys example
    - Phone Numbers
    - Names
    - Employee Id [E1232]

-------
How Hash Functions Work ?
    -> Should always map a large key to same small value
    -> Should generate values from 0 to m-1
    -> Should be fast, O(1) for integer and O(Len) for string of length 'len' (For string : we have to uniformly process all the character in hash function)
    -> Should uniformly distribute large keys into hash table slots

--------
Collision Handling

If we know keys in advance, then we can use Perfect Hashing that gurantees no collision.
If we don't know keys, then we use of the following
    -> Chaning
        We chain the colliding items
            We use array of Linked List. So when the collision happens, insert the item at the end of Linked list. Normally we should use prime number while designing hash function that is doing mode(%) etc  this will reduce the chances of collision.
        Instead of Linked List we can also use

    -> Open Addressing
    We use the same array
        -> Linear Probing
        -> Quadratic Probing
        -> Double Hashing
    