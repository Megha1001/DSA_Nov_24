Asymptotic Ananlysis
----------------------
-> The idea is to measure order of growth. (We meaure order of growth of time taken by a functional program, in terms of input size.)
-> Doesn't depend upon machine, programming language etc.
-> No need to implement, we can analyze algorithm.

Assumptions
------------
-> Mathematical operations like addition, substraction, division, multiplication etc taken same time (whether you add 1+1 or 10000000+100000000, its consider to take same time)


Direct way to find and compare growths
------------------------------------------
- Ignore lower order terms
- Ignore leading term constant

How to compare terms ?
-----------------------
c < logLogN < logN < N^1/3 < N^1/2 < N < N^2 < N^3 < N^4 < 2^N < N^N

------------
The algorithm whose Order of growth is higher is bad algorithm
------------

Asymptotic Notation
--------------------
Big O : Exact or Upper
Theta : Exact
Omega : Exact or lower

------
When we know exact order of growth use Theta notation not Big O

------

Recursion Tree Method for solving reccurrences
- We consider the recursion tree and compute total work done.
- We write non recursive part as root of the tree and write the recursive part as children
- We keep expanding until we see a pattern

-------

Space complexity 
- Order of growth of memory ( or RAM) space in terms of input size

Auxiliary Space
- Order of growth of extra space or temporary space in terms of input size. (space created other than input and output)
    - Stack call consideration
- Its basically equals to the height of the tree ( maximum length from root to leaf path )



------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------
Euclidean Algorithm
---------------------
1. Let 'b' be smaller than 'a'
     gcd(a, b) = gcd(a-b, b)

why ? 
    Let 'g' e GCD of 'a' and 'b'
    a = gx , b = gy and GCD(x, y) = 1
    (a-b) = g(x-y) ==> GCD of 1-b has be to 'g' otherwise it must have some common factor b/w x, y that contradicts GCD(x, y)=1
    Hence 
    gcd(a, b) = gcd(a-b, b) => g

Optimized euclidean algorith --> Time Complexity : O(log(min(a,b)))
Given number -> a,b
    if(b==0) return a;

    return gcd(b, a%b)


--------
LCM Algo
LCM is the smallest number that divides both the numbers.
a*b = LCM(a, b) * gcd(a, b)
---------

Prime number : Number that are only divisible by 1 and itself
Note : 1 is not the prime number ( nor composite number(non prime are termed as composite))
2 is the only even prime number


Idea : Divisors always appears in pair

30 -> (1,30) , (2, 15), (3,10), (5, 6)

=> if (x,y) is a pair
    x*y = n 
    and if x<=y
    x*x <=n
    => x <= Sqrt(n)

- Sieve of Eratosthenes

-----------------
Computing power

power(x,n) = power(x,n/2)*power(x,n/2), if n is even
power(x, n) = power(x,n/2)*power(x,n/2) * x, if n is odd

---------------
BitWise Operators
--------------------
1. Negation/ Bitwise not(~)
Positive numbers -> represent in decimal -> binary
Negative numbers -> represet in 2's complement


2's complement representation of -x = 2^32 - x


all 1s = 2^32-1


left shift operator(<<)
------------------
Signed and unsigned -> doesn't matter
Operation: The left shift operator simply shifts bits to the left by a specified number of positions.
Effect: Each left shift effectively multiplies the number by 2, regardless of whether the number is signed or unsigned.
No Sign Consideration: Since left shifting always adds zeros to the right, it does not affect the sign bit. As a result, the outcome is the same for both signed and unsigned integers when using the left shift operator.


Right Shift Operator (>>)
--------------------------
Syntax: value >> n
Function: Shifts the bits of value to the right by n positions. Each shift to the right effectively divides the number by 2.

Signed vs. Unsigned:

Signed Right Shift: Preserves the sign of the number. For a negative number, it fills the leftmost bits with 1.
Example: For a signed integer, -4 >> 1 (binary 11111100 for -4 in 8-bit) becomes -2 (binary 11111110).

Unsigned Right Shift: Does not preserve the sign. It always fills the leftmost bits with 0, regardless of the original sign.
Example: For an unsigned integer, -4 >> 1 (interpreting -4 as 252 in 8-bit) becomes 126 (binary 01111110).

Unsigned Right Shift Operator (>>>)
Syntax: value >>> n
Function: Performs a right shift but always fills the leftmost bits with 0, treating the number as an unsigned integer.
Example: In Java, if -1 >>> 1 results in 2147483647 (binary 01111111111111111111111111111111).

Summary
Left Shift (<<): Multiplies by powers of two.
Right Shift (>>): Preserves sign for signed integers; divides by powers of two.
Unsigned Right Shift (>>>): Ignores sign, always fills with 0.


----------------

Binary representation of -ve numbers
- Negative numbers are represented in 2's complement form

- > Range of numbers : [-2^(n-1) to 2^(n-1) - 1], here n are number of bits

-> steps to find 2's complement : 
    a. invert all the bits
    b. add 1
    Direct formula = 2^n - x

-> Easiest way Steps to Find 2's Complement by Traversing:)
Start from the leftmost bit and traverse the binary number to find the first occurrence of 1.
Copy all bits as they are until you encounter the first 1.


Why 2's complement form
-ve numbers are represented using 2's complement form . +ve numbers uses simple decimal to binary conversion

with other available representation(signed bit, 1's complement -> we have two representation for zero, in 2's complement we have only one representation of zero)


-> Signed Number -> Range 0 to 2^n -1 -> Support by C++, Python, C,  (supports both signed and unsigned)
-> Unsigned Number -> -2^(n-1) to 2^(n-1) -1 --> Support by JAVA

REASON 1 :-> We have only one representation of zero
REASON 2 :-> Intuition behind 2's complement
    -> THe arithmetic operators are easier to perform. Actually 2's complment form is derived from the idea of 0-x

REASON 3 :-> The leading bit is always zero( here we are only talking about -ve numbers, for +ve we are anyway using n-1 bit so leading bit is always 0)


---------------------------------------------
                    RECURSION
---------------------------------------------
If we have solution to smaller problems we can use those solution to solve the bigger problem.



TAIL RECURSION : A function is called tail recursive when the parent function has nothin gmore to do when the child function finish its execution.
In simple words, a  function is tail recursive when the last thing that happens in the function is recursive call and nothing happens after that.

Ex :
void fun(int n){
    if(n==0){
        return;
    }
    print(n)
    fun(n-1);
}

Why is this function execution faster ?
The point is, your caller doesn't have to save the state because in recursion. So what modern compilers do if they see function is tail recursive they change the function 


void fun(int n){
    start:
    if(n==0){
        return;
    }
    print(n)
    n = n-1;
    goto start
}

These are the changes they make internally --> they remove recursive completely. The changes that modern compiler makes is call TAIL CALL ELIMINATION

Notes
 : Quick Sort is tail recursive and merge sort is not . That is one of the reason why Quick sort is preferred
 : We can't convert every recursive to tail recursive
 : Tree Traversal
    : PreOrder traversal : tail recursive for right child
    : InOrder traversal : tail recursive for right child
    : PostOrder traversal : not tail recursive
    If we have a choice to do question then we should always try to do that using either preorder or inorder traversal since these are tail recursive

# Base Case
Base cases in recursion are those cases/inputs for which we can't break the problem into smaller such problems

------------------
Subsets/subsequences : String obtained by removing 0 or more characters from the given string and keeping the order of remaning characs as it is.

For the length of string n there can be 2^n subsets

------------------------------------------------------------
########################ARRAY########################
------------------------------------------------------------

- Contiguous memory
- accessed using index, that begin with 0.

Advantages
- Random access (Since elements are stored at contiguous location, we can get ith element in constant time, arr[i])
- Cache Friendliness

Types
- Fixed sized array

- Dynamic sized array : ArrayList

Note : The elements of an ArrayList are not stored in contiguous locations, 
Memory allocation for both fixed and dynamic size array is heap allocated

Its always recommened to use dynamic size array (You can initialize the initial size)